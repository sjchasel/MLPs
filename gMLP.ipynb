{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:11:29.688127Z",
     "start_time": "2021-06-29T09:11:29.629297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nB: batch_size\\nL: seq_len\\nD: d_model\\nH: d_ffn\\nV: vocab_size\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "B: batch_size\n",
    "L: seq_len\n",
    "D: d_model\n",
    "H: d_ffn\n",
    "V: vocab_size\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:18:53.643881Z",
     "start_time": "2021-06-29T09:18:53.597007Z"
    }
   },
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GELU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1+F.tanh(np.sqrt(2/np.pi)*(x+0.044715*torch.pow(x,3))))\n",
    "# https://blog.csdn.net/w137093940/article/details/112756141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:18:55.244600Z",
     "start_time": "2021-06-29T09:18:55.200718Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpatialGatingUnit(nn.Module):\n",
    "    def __init__(self, seq_len, d_ffn):\n",
    "        super().__init__()\n",
    "        # 输入的x的维度是B * L *H ,那么LayerNorm的参数是H(与x的最后一个维度相同)\n",
    "        self.norm = nn.LayerNorm(d_ffn//2)\n",
    "        self.proj = nn.Conv1d(seq_len, seq_len, kernel_size=1) \n",
    "        # 输入通道是seq_len，输出通道是seq_len（卷积核个数），卷积核大小为1\n",
    "        nn.init.constant_(self.proj.weight, 0)\n",
    "        nn.init.constant_(self.proj.bias, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # x B * L * H\n",
    "        u, v = x.chunk(2, dim=-1)  # 在最后一个维度上切分\n",
    "        # u,v B * L * H/2\n",
    "        v = self.norm(v)\n",
    "        v = self.proj(v) # v B * L * H/2\n",
    "        return u * v  # B * L * H/2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:19:05.166069Z",
     "start_time": "2021-06-29T09:19:05.118197Z"
    }
   },
   "outputs": [],
   "source": [
    "class gMLPblock(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, d_ffn):\n",
    "        # seq_len是序列的长度\n",
    "        # d_model是词向量的维度\n",
    "        # d_ffn是前馈神经网络中中间层的维度\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.channel_proj1 = nn.Linear(d_model, d_ffn)\n",
    "        self.sgu = SpatialGatingUnit(seq_len, d_ffn)\n",
    "        self.channel_proj2 = nn.Linear(d_ffn//2, d_model)\n",
    "        self.act = GELU()\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        # 输入的x的维度是 B * L * D\n",
    "        shortcut = x\n",
    "        x = self.norm(x)\n",
    "        x = self.channel_proj1(x)  # U d_model * seq_len x变为batch_size * seq_len * d_ffn\n",
    "        x = self.act(x)  # x batch_size * seq_len * d_ffn\n",
    "        x = self.sgu(x)  # x batch_size * seq_len * d_ffn/2\n",
    "        x = self.channel_proj2(x)  # V batch_size * seq_len * d_model\n",
    "        return shortcut + x # batch_size * seq_len * d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:19:09.743828Z",
     "start_time": "2021-06-29T09:19:09.715900Z"
    }
   },
   "outputs": [],
   "source": [
    "class gMLP(nn.Module):\n",
    "    def __init__(self, seq_len=256, d_model=256, d_ffn=512, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(*[gMLPblock(seq_len, d_model, d_ffn)]*num_layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:19:11.110173Z",
     "start_time": "2021-06-29T09:19:11.076265Z"
    }
   },
   "outputs": [],
   "source": [
    "class gMLPofLanguageModel(gMLP):\n",
    "    def __init__(self, vocab_size=20000, seq_len=256, d_model=256, d_ffn=512, num_layers=6):\n",
    "        super().__init__(seq_len, d_model, d_ffn, num_layers)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)  # B * L * D\n",
    "        out = self.model(emb) # B * L * D\n",
    "        out = self.fc(out) # B * L * V\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:19:28.494686Z",
     "start_time": "2021-06-29T09:19:24.749702Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12968\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 49, 10000])\n"
     ]
    }
   ],
   "source": [
    "num_tokens=10000\n",
    "bs=50\n",
    "len_sen=49\n",
    "num_layers=6\n",
    "input=torch.randint(num_tokens,(bs,len_sen)) #bs,len_sen\n",
    "gmlp = gMLPofLanguageModel(vocab_size=num_tokens,seq_len=len_sen,d_model=512,d_ffn=1024)\n",
    "output=gmlp(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:25:48.759424Z",
     "start_time": "2021-06-29T09:25:48.734493Z"
    }
   },
   "outputs": [],
   "source": [
    "class TinyAttn(nn.Module):\n",
    "    def __init__(self, d_out,d_model,d_attn=64):\n",
    "        super().__init__()\n",
    "        self.proj1 = nn.Linear(d_model, 3 * d_attn)\n",
    "        self.proj2 = nn.Linear(d_attn, d_out)\n",
    "        self.d_attn = d_attn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        qkv = self.proj1(x) # B * L * 3attn\n",
    "        q, k, v = qkv.chunk(3, dim=-1) # B * L * attn\n",
    "        k = k.permute(0,2,1) # B * attn *L\n",
    "        w = torch.matmul(q,k) # B * L * L\n",
    "        a = F.softmax(w*torch.rsqrt(d_attn)) # B * L * L\n",
    "        x = torch.matmul(a,v) # B * L * attn\n",
    "        x = self.proj2(x) # B * L * d_out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:25:50.373995Z",
     "start_time": "2021-06-29T09:25:50.346107Z"
    }
   },
   "outputs": [],
   "source": [
    "class aSpatialGatingUnit(nn.Module):\n",
    "    def __init__(self, seq_len, d_ffn, d_model):\n",
    "        super().__init__()\n",
    "        # 输入的x的维度是B * L *H ,那么LayerNorm的参数是H(与x的最后一个维度相同)\n",
    "        self.norm = nn.LayerNorm(d_ffn//2)\n",
    "        self.proj = nn.Conv1d(seq_len, seq_len, kernel_size=1) \n",
    "        # 输入通道是seq_len，输出通道是seq_len（卷积核个数），卷积核大小为1\n",
    "        nn.init.constant_(self.proj.weight, 0)\n",
    "        nn.init.constant_(self.proj.bias, 1)\n",
    "        self.attn = TinyAttn(d_out=d_ffn // 2, d_model=d_ffn)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        attn_out = self.attn(x)\n",
    "        # x B * L * H\n",
    "        # attn_out B * L * d_ffn/2\n",
    "        u, v = x.chunk(2, dim=-1)  # 在最后一个维度上切分\n",
    "        # u,v B * L * H/2\n",
    "        v = self.norm(v)\n",
    "        v = self.proj(v) # v B * L * H/2\n",
    "        v = v + attn_out\n",
    "        return u * v  # B * L * H/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:25:51.716584Z",
     "start_time": "2021-06-29T09:25:51.689661Z"
    }
   },
   "outputs": [],
   "source": [
    "class aMLPblock(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, d_ffn):\n",
    "        # seq_len是序列的长度\n",
    "        # d_model是词向量的维度\n",
    "        # d_ffn是前馈神经网络中隐藏层的维度\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.channel_proj1 = nn.Linear(d_model, d_ffn)\n",
    "        self.sgu = aSpatialGatingUnit(seq_len, d_ffn, d_model)\n",
    "        self.channel_proj2 = nn.Linear(d_ffn//2, d_model)\n",
    "        self.act = GELU()\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        # 输入的x的维度是 B * L * D\n",
    "        shortcut = x\n",
    "        x = self.norm(x)\n",
    "        x = self.channel_proj1(x)  # U d_model * seq_len x变为batch_size * seq_len * d_ffn\n",
    "        x = self.act(x)  # x batch_size * seq_len * d_ffn\n",
    "        x = self.sgu(x)  # x batch_size * seq_len * d_ffn/2\n",
    "        x = self.channel_proj2(x)  # V batch_size * seq_len * d_model\n",
    "        return shortcut + x # batch_size * seq_len * d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:25:53.038281Z",
     "start_time": "2021-06-29T09:25:53.020320Z"
    }
   },
   "outputs": [],
   "source": [
    "class aMLP(nn.Module):\n",
    "    def __init__(self, seq_len=256, d_model=256, d_ffn=512, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(*[aMLPblock(seq_len, d_model, d_ffn)]*num_layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:25:54.549971Z",
     "start_time": "2021-06-29T09:25:54.530077Z"
    }
   },
   "outputs": [],
   "source": [
    "class aMLPofLanguageModel(aMLP):\n",
    "    def __init__(self, vocab_size=20000, seq_len=256, d_model=256, d_ffn=512, num_layers=6):\n",
    "        super().__init__(seq_len, d_model, d_ffn, num_layers)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)  # B * L * D\n",
    "        out = self.model(emb) # B * L * D\n",
    "        out = self.fc(out) # B * L * V\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T09:25:58.933433Z",
     "start_time": "2021-06-29T09:25:55.618294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 49, 10000])\n"
     ]
    }
   ],
   "source": [
    "num_tokens=10000\n",
    "bs=50\n",
    "len_sen=49\n",
    "num_layers=6\n",
    "input=torch.randint(num_tokens,(bs,len_sen)) #bs,len_sen\n",
    "gmlp = gMLPofLanguageModel(vocab_size=num_tokens,seq_len=len_sen,d_model=512,d_ffn=1024)\n",
    "output=gmlp(input)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
